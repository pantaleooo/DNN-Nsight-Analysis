import time
import torch
import numpy as np
import pandas as pd
import multiprocessing
import threading
import queue
import signal
import os
import random

# --- 1. å®éªŒé…ç½® (å¯è°ƒæ•´çš„å‚æ•°) ---

CONFIG = {
    # å®éªŒæ—¶é•¿
    "EXPERIMENT_DURATION_S": 30.0,
    
    # æ¨ç†ä»»åŠ¡ (T_I) é…ç½®
    "INFERENCE_CONCURRENCY": 4,      # æ¨¡æ‹Ÿå¹¶å‘çš„æ¨ç†è¯·æ±‚æ•°
    "SLO_MS": 100.0,                 # æ¨ç†SLOç›®æ ‡ (æ¯«ç§’)
    
    # P_I (æ¨ç†é¢„å¤„ç†) - CPUå¯†é›†å‹
    "P_I_CPU_WORK_ITERATIONS": 20_000_000, # è°ƒæ•´æ­¤å€¼ä»¥æ¨¡æ‹Ÿçº¦ 20-30ms çš„CPUå·¥ä½œ
    
    # E_I (æ¨ç†æ‰§è¡Œ) - GPUå¯†é›†å‹
    "E_I_GPU_WORK_MATRIX_SIZE": 2048,   # è°ƒæ•´æ­¤å€¼ä»¥æ¨¡æ‹Ÿçº¦ 30-50ms çš„GPUå·¥ä½œ

    # é‡è®­ç»ƒä»»åŠ¡ (T_R) é…ç½®
    "RETRAINING_INTERVAL_S": 7.0,     # æ¯éš”å¤šä¹…è§¦å‘ä¸€æ¬¡é‡è®­ç»ƒ
    
    # P_R (é‡è®­ç»ƒé¢„å¤„ç†) - CPUå¯†é›†å‹
    "P_R_CPU_WORK_ITERATIONS": 150_000_000, # è°ƒæ•´æ­¤å€¼ä»¥æ¨¡æ‹Ÿçº¦ 2-3 ç§’çš„é‡åº¦CPUå·¥ä½œ

    # P_R (é‡è®­ç»ƒé¢„å¤„ç†) - GPUå¯†é›†å‹ (ç”¨äºç­–ç•¥C)
    "P_R_GPU_WORK_MATRIX_SIZE": 6144,   # è°ƒæ•´æ­¤å€¼ä»¥æ¨¡æ‹Ÿåœ¨GPUä¸Š 1-2 ç§’çš„å·¥ä½œ
    
    # E_R (é‡è®­ç»ƒæ‰§è¡Œ) - GPUå¯†é›†å‹
    "E_R_GPU_WORK_MATRIX_SIZE": 12288,  # è°ƒæ•´æ­¤å€¼ä»¥æ¨¡æ‹Ÿ 4-5 ç§’çš„é‡åº¦GPUè®­ç»ƒ
}

# --- 2. èµ„æºæ¨¡æ‹Ÿå·¥å…· ---

def simulate_cpu_work(log_queue, tag, iterations):
    """é€šè¿‡æ‰§è¡Œæ•°å­¦è¿ç®—æ¥çœŸå®åœ°æ¶ˆè€—CPUæ—¶é—´"""
    pid = os.getpid()
    start_time = time.monotonic()
    
    # å¿™ç­‰å¾…å¾ªç¯ï¼šè¿™æ˜¯æ¨¡æ‹ŸCPUå¯†é›†å‹å·¥ä½œçš„å…³é”®
    _ = 0
    for i in range(iterations):
        _ = (i * i + 123.456) * (789.012 - i)
        
    end_time = time.monotonic()
    duration_ms = (end_time - start_time) * 1000
    
    log_queue.put({
        "timestamp": end_time,
        "type": tag,
        "pid": pid,
        "duration_ms": duration_ms
    })
    return duration_ms

def simulate_gpu_work(log_queue, tag, matrix_size):
    """é€šè¿‡æ‰§è¡ŒçŸ©é˜µä¹˜æ³•æ¥çœŸå®åœ°æ¶ˆè€—GPUæ—¶é—´"""
    pid = os.getpid()
    if not torch.cuda.is_available():
        log_queue.put({
            "timestamp": time.monotonic(),
            "type": "ERROR",
            "pid": pid,
            "message": "CUDA not available. GPU work simulated with sleep."
        })
        # å¦‚æœæ²¡æœ‰GPUï¼Œåˆ™é™çº§ä¸ºsleepï¼Œä½†è¿™ä¸ä¼šæ¶ˆè€—èµ„æº
        time.sleep(matrix_size / 2048.0 * 0.05) 
        return
        
    device = torch.device("cuda")
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)

    try:
        # åœ¨GPUä¸Šåˆ›å»ºæ•°æ®
        a = torch.randn(matrix_size, matrix_size, device=device)
        b = torch.randn(matrix_size, matrix_size, device=device)
        
        start_event.record()
        # æ‰§è¡ŒGPUå¯†é›†å‹å·¥ä½œ
        c = torch.matmul(a, b)
        end_event.record()
        
        # å…³é”®ï¼šåŒæ­¥CPUå’ŒGPUï¼Œç¡®ä¿GPUå·¥ä½œå·²å®Œæˆ
        torch.cuda.synchronize()
        duration_ms = start_event.elapsed_time(end_event)
        
        log_queue.put({
            "timestamp": time.monotonic(),
            "type": tag,
            "pid": pid,
            "duration_ms": duration_ms
        })
        return duration_ms

    except torch.cuda.OutOfMemoryError:
        log_queue.put({
            "timestamp": time.monotonic(),
            "type": "ERROR",
            "pid": pid,
            "message": f"CUDA OOM with matrix size {matrix_size}. Reduce workload."
        })
    except Exception as e:
        log_queue.put({
            "timestamp": time.monotonic(),
            "type": "ERROR",
            "pid": pid,
            "message": f"GPU work failed: {e}"
        })

# --- 3. å¹¶å‘å·¥ä½œæµ (Workers) ---

def inference_worker(request_id, log_queue, stop_event):
    """
    æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„ç«¯åˆ°ç«¯æ¨ç†è¯·æ±‚ (T_I = P_I + E_I)
    åœ¨ä¸€ä¸ªå•ç‹¬çš„è¿›ç¨‹ä¸­è¿è¡Œã€‚
    """
    if stop_event.is_set():
        return
        
    pid = os.getpid()
    e2e_start = time.monotonic()
    
    # é˜¶æ®µ 1: æ¨ç†é¢„å¤„ç† (CPU)
    p_i_duration = simulate_cpu_work(
        log_queue, 
        "P_I", 
        CONFIG["P_I_CPU_WORK_ITERATIONS"]
    )
    
    # é˜¶æ®µ 2: æ¨ç†æ‰§è¡Œ (GPU)
    e_i_duration = simulate_gpu_work(
        log_queue, 
        "E_I", 
        CONFIG["E_I_GPU_WORK_MATRIX_SIZE"]
    )
    
    e2e_end = time.monotonic()
    e2e_duration_ms = (e2e_end - e2e_start) * 1000
    
    # ä¸ŠæŠ¥ç«¯åˆ°ç«¯å»¶è¿Ÿ
    log_queue.put({
        "timestamp": e2e_end,
        "type": "T_I_E2E",
        "pid": pid,
        "request_id": request_id,
        "duration_ms": e2e_duration_ms,
        "p_i_duration_ms": p_i_duration,
        "e_i_duration_ms": e_i_duration,
        "slo_violation": e2e_duration_ms > CONFIG["SLO_MS"]
    })

def load_generator(log_queue, stop_event):
    """
    æ¨¡æ‹Ÿä¸€ä¸ªå…·æœ‰å›ºå®šå¹¶å‘æ•°(CONCURRENCY)çš„æ¨ç†è´Ÿè½½ç”Ÿæˆå™¨ã€‚
    è¿™æ¨¡æ‹Ÿäº†ä¸€ä¸ªâ€œé—­ç¯â€ç³»ç»Ÿï¼Œæ€»æ˜¯æœ‰Nä¸ªè¯·æ±‚åœ¨å¤„ç†ä¸­ã€‚
    """
    request_id = 0
    pool = multiprocessing.Pool(processes=CONFIG["INFERENCE_CONCURRENCY"])
    
    while not stop_event.is_set():
        try:
            # æŒç»­æäº¤ä»»åŠ¡ä»¥ä¿æŒå¹¶å‘æ°´å¹³
            pool.apply_async(inference_worker, (request_id, log_queue, stop_event))
            request_id += 1
            # å¢åŠ ä¸€ä¸ªå°çš„éšæœºå»¶è¿Ÿï¼Œä½¿è¯·æ±‚åˆ°è¾¾ä¸é‚£ä¹ˆå‡åŒ€
            time.sleep(random.uniform(0.001, 0.01)) 
        except Exception as e:
            if not stop_event.is_set():
                print(f"[LoadGen] Error: {e}")
                
    pool.close()
    pool.join()
    print("[LoadGen] Shutting down.")

def retraining_worker(policy, log_queue):
    """
    æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„é‡è®­ç»ƒä»»åŠ¡ (T_R = P_R + E_R)
    ç­–ç•¥ B: P_R on CPU
    ç­–ç•¥ C: P_R on GPU
    """
    pid = os.getpid()
    log_queue.put({
        "timestamp": time.monotonic(),
        "type": "T_R_START",
        "pid": pid,
        "policy": policy
    })
    
    p_r_duration, e_r_duration = 0, 0
    
    if policy == "B_CPU_BLIND":
        # --- ç­–ç•¥ B: é‡è®­ç»ƒé¢„å¤„ç†åœ¨ CPU ä¸Šè¿è¡Œ ---
        # è¿™å°†ä¸ P_I (æ¨ç†é¢„å¤„ç†) å‘ç”Ÿå†²çª
        p_r_duration = simulate_cpu_work(
            log_queue, 
            "P_R", 
            CONFIG["P_R_CPU_WORK_ITERATIONS"]
        )
    
    elif policy == "C_PACS_LIKE":
        # --- ç­–ç•¥ C: é‡è®­ç»ƒé¢„å¤„ç†åœ¨ GPU ä¸Šè¿è¡Œ ---
        # è¿™å°†ä¸ E_I (æ¨ç†æ‰§è¡Œ) å‘ç”Ÿå†²çªï¼Œä½†ä¿æŠ¤äº†CPU
        p_r_duration = simulate_gpu_work(
            log_queue, 
            "P_R", 
            CONFIG["P_R_GPU_WORK_MATRIX_SIZE"]
        )
    
    # ä¸¤ä¸ªç­–ç•¥éƒ½åœ¨GPUä¸Šæ‰§è¡Œé‡è®­ç»ƒ
    e_r_duration = simulate_gpu_work(
        log_queue, 
        "E_R", 
        CONFIG["E_R_GPU_WORK_MATRIX_SIZE"]
    )
    
    log_queue.put({
        "timestamp": time.monotonic(),
        "type": "T_R_END",
        "pid": pid,
        "p_r_duration_ms": p_r_duration,
        "e_r_duration_ms": e_r_duration
    })

def log_processor(log_queue, stop_event, all_logs):
    """
    ä¸€ä¸ªå•ç‹¬çš„çº¿ç¨‹ï¼Œç”¨äºå®‰å…¨åœ°ä»é˜Ÿåˆ—ä¸­æ”¶é›†æ‰€æœ‰æ—¥å¿—ã€‚
    """
    while not stop_event.is_set():
        try:
            log_entry = log_queue.get(timeout=0.1)
            all_logs.append(log_entry)
        except queue.Empty:
            # æ£€æŸ¥ stop_event æ˜¯å¦å·²è®¾ç½®ï¼Œä»¥ä¾¿åœ¨ä¸»çº¿ç¨‹ç»“æŸåé€€å‡º
            if stop_event.is_set() and log_queue.empty():
                break
        except Exception as e:
            print(f"[LogProcessor] Error: {e}")
            
    # æ¸…ç©ºé˜Ÿåˆ—ä¸­å‰©ä½™çš„æ—¥å¿—
    while not log_queue.empty():
        try:
            all_logs.append(log_queue.get_nowait())
        except queue.Empty:
            break
    print("[LogProcessor] Shutting down.")


def run_experiment(policy):
    """
    æ‰§è¡Œå•ä¸ªç­–ç•¥å®éªŒçš„ä¸»å‡½æ•°ã€‚
    """
    print("\n" + "="*50)
    print(f"ğŸš€ [Experiment] Staging Policy: {policy}")
    print("="*50)

    # multiprocessing.Manager ç”¨äºåœ¨è¿›ç¨‹é—´å…±äº« `all_logs` åˆ—è¡¨
    manager = multiprocessing.Manager()
    all_logs = manager.list()
    log_queue = manager.Queue()
    stop_event = manager.Event()

    # å¯åŠ¨æ—¥å¿—æ”¶é›†å™¨
    log_thread = threading.Thread(target=log_processor, args=(log_queue, stop_event, all_logs))
    log_thread.start()

    # å¯åŠ¨æ¨ç†è´Ÿè½½ç”Ÿæˆå™¨
    load_gen_proc = multiprocessing.Process(target=load_generator, args=(log_queue, stop_event))
    load_gen_proc.start()
    
    print(f"[Main] Inference load started with {CONFIG['INFERENCE_CONCURRENCY']} concurrent workers.")
    
    # --- ç­–ç•¥é€»è¾‘ ---
    retraining_procs = []
    
    if policy == "A_BASELINE":
        # ç­–ç•¥ A: åªè¿è¡Œæ¨ç†
        print("[Main] Running BASELINE. No retraining will be triggered.")
        time.sleep(CONFIG["EXPERIMENT_DURATION_S"])
        
    elif policy in ["B_CPU_BLIND", "C_PACS_LIKE"]:
        # ç­–ç•¥ B & C: å‘¨æœŸæ€§åœ°è§¦å‘é‡è®­ç»ƒ
        start_time = time.monotonic()
        while time.monotonic() - start_time < CONFIG["EXPERIMENT_DURATION_S"]:
            # æ¸…ç†å·²å®Œæˆçš„é‡è®­ç»ƒè¿›ç¨‹
            retraining_procs = [p for p in retraining_procs if p.is_alive()]
            
            # æ¨¡æ‹Ÿâ€œè°ƒåº¦å™¨â€è§¦å‘
            if not retraining_procs: # ä»…å½“æ²¡æœ‰é‡è®­ç»ƒåœ¨è¿è¡Œæ—¶
                print(f"[Main] Triggering retraining task for policy {policy}...")
                p = multiprocessing.Process(target=retraining_worker, args=(policy, log_queue))
                p.start()
                retraining_procs.append(p)
                
            time.sleep(CONFIG["RETRAINING_INTERVAL_S"])
            
    # --- å®éªŒç»“æŸï¼Œå¼€å§‹æ¸…ç† ---
    print("\n[Main] Experiment duration ended. Signaling all processes to stop...")
    stop_event.set()

    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹ç»“æŸ
    load_gen_proc.join(timeout=10)
    if load_gen_proc.is_alive():
        print("[Main] Forcing load generator termination.")
        load_gen_proc.terminate()
        
    for p in retraining_procs:
        p.join(timeout=5)
        if p.is_alive():
            p.terminate()
            
    # ç­‰å¾…æ—¥å¿—æ”¶é›†å™¨å®Œæˆ
    log_thread.join(timeout=5)
    
    print(f"[Main] Experiment for {policy} finished. Processing {len(all_logs)} log entries.")
    
    # --- 4. æŠ¥å‘Šç»“æœ ---
    if not all_logs:
        print("[Error] No logs were collected.")
        return

    # è½¬æ¢ä¸º Pandas DataFrame è¿›è¡Œåˆ†æ
    df = pd.DataFrame(list(all_logs))
    
    # æå–å…³é”®çš„æ¨ç†æ€§èƒ½æ•°æ®
    e2e_df = df[df["type"] == "T_I_E2E"].copy()
    
    if e2e_df.empty:
        print("[Error] No E2E inference logs found.")
        return

    e2e_df["duration_ms"] = pd.to_numeric(e2e_df["duration_ms"])
    
    # è®¡ç®—å…³é”®æŒ‡æ ‡
    avg_latency = e2e_df["duration_ms"].mean()
    p95_latency = e2e_df["duration_ms"].quantile(0.95)
    p99_latency = e2e_df["duration_ms"].quantile(0.99)
    total_requests = len(e2e_df)
    slo_violations = e2e_df["slo_violation"].sum()
    slo_violation_rate = (slo_violations / total_requests) * 100
    throughput = total_requests / CONFIG["EXPERIMENT_DURATION_S"]

    print("\n" + "-"*50)
    print(f"ğŸ“Š [Results] Report for Policy: {policy}")
    print(f"   Total Requests Served: {total_requests:,.0f}")
    print(f"   Avg. Throughput (req/s): {throughput:.2f}")
    print(f"   Avg. E2E Latency (ms): {avg_latency:.2f}")
    print(f"   p95 E2E Latency (ms): {p95_latency:.2f}")
    print(f"   p99 E2E Latency (ms): {p99_latency:.2f}")
    print(f"   SLO Violations (> {CONFIG['SLO_MS']} ms): {slo_violations:,.0f}")
    print(f"   SLO Violation Rate: {slo_violation_rate:.2f} %")
    print("-" * 50)
    
    # è¿”å› p99 å»¶è¿Ÿä»¥è¿›è¡Œè·¨ç­–ç•¥æ¯”è¾ƒ
    return p99_latency


# --- 5. ä¸»æ‰§è¡Œå‡½æ•° ---

def main():
    # ç¡®ä¿å­è¿›ç¨‹åœ¨ CUDA ä¸Šæ˜¯å®‰å…¨çš„
    multiprocessing.set_start_method("spawn", force=True)

    # æ£€æŸ¥ CUDA
    if not torch.cuda.is_available():
        print("="*50)
        print("âš ï¸ WARNING: No CUDA GPU detected. âš ï¸")
        print("   GPU work will be simulated with time.sleep().")
        print("   This will NOT accurately demonstrate CPU vs GPU contention.")
        print("   Please run on a machine with a CUDA-enabled GPU.")
        print("="*50)
    else:
        print(f"âœ… Found CUDA Device: {torch.cuda.get_device_name(0)}")

    # è¿è¡Œä¸‰ä¸ªç­–ç•¥
    results = {}
    results["A_BASELINE"] = run_experiment("A_BASELINE")
    results["B_CPU_BLIND"] = run_experiment("B_CPU_BLIND")
    results["C_PACS_LIKE"] = run_experiment("C_PACS_LIKE")

    print("\n\n" + "#"*60)
    print("### Final Experiment Summary (p99 Latency) ###")
    print(f"SLO Target: {CONFIG['SLO_MS']:.2f} ms")
    print(f"  Policy A (Baseline):   {results['A_BASELINE']:.2f} ms")
    print(f"  Policy B (CPU-Blind):  {results['B_CPU_BLIND']:.2f} ms   <--- é¢„æœŸæ­¤å€¼æœ€é«˜ (CPUå†²çª)")
    print(f"  Policy C (PACS-like):  {results['C_PACS_LIKE']:.2f} ms   <--- é¢„æœŸæ­¤å€¼æ¥è¿‘åŸºçº¿ (å†²çªå·²è§£å†³)")
    print("#"*60)

if __name__ == "__main__":
    main()
